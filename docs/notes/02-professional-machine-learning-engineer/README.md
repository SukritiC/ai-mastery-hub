# Professional Machine Learning Engineer - Introduction  
ğŸ“ <a href='https://fern-stop-81f.notion.site/Machine-Learning-Engineer-Learning-Path-Introduction-1b313f9f5c038013840bf69971e5759e?pvs=74'> Read Here </a> 

## ğŸ—’ï¸ Summary
This guide provides a concise overview of modern AI development using Google Cloud. It covers LLM Agents, Generative AI, and tools like Vertex AI Studio, along with Google Cloud Infrastructure. Key topics include ML categories, BigQuery for data storage and analytics, and various AI development options such as pre-trained and generative AI APIs. The guide also outlines the end-to-end AI development workflow, with a focus on building and deploying models efficiently using Vertex AI.

## ğŸ“š Table of Contents

1. LLM Agents
2. Generative AI
3. Vertex AI Sudio
4. Google Cloud Infrastructure
5. ML Categories
6. Big Query: storage & analytics
7. AI Developement Options
8. Pre-trained APIs
9. Generative AI APIs
10. Vertex AI
11. AI Development Workflow
    

---

# Professional Machine Learning Engineer - Data Preparation & Modelling  
ğŸ“˜ <a href='https://fern-stop-81f.notion.site/Machine-Learning-Engineer-Learning-Path-Data-Preparation-Modeling-1c113f9f5c03808f990ddb748350f5dc'> Read Here </a> 

## ğŸ—’ï¸ Summary
This content explores key Google Cloud Platform (GCP) tools and services for data engineering and machine learning. It starts by comparing Dataflow and Dataproc, highlighting their use cases in stream and batch processing. The Pub/Sub to BigQuery template demonstrates how to set up real-time data pipelines. The Cloud Natural Language API showcases how to extract insights from text using machine learning. Using Vertex AI notebooks, users can develop and experiment with ML models efficiently. Hands-on sessions with BigQuery ML guide users in building models directly within BigQuery and applying them to practical scenarios, such as predicting visitor purchases using classification models.

## ğŸ“š Table of Contents

1. GCP Dataflow vs Dataproc
2. Pub/sub to BigQery Template
3. Cloud Natural Language API
4. Working with notebooks in Vertex AI
5. Creating ML Models with BigQuery ML - Hands On
6. Predict Visitor Purchases with a Classification Model in BigQuery ML - Hands On
   

---

# Build, Train, and Deploy ML Models with Keras on Google Cloud

ğŸ“˜ <a href='https://fern-stop-81f.notion.site/Build-Train-and-Deploy-ML-Models-with-Keras-on-Google-Cloud-20813f9f5c0380bcb67deec01556e50f'> Read Here </a> 

## ğŸ—’ï¸ Summary
This module introduces the TensorFlow ecosystem, covering its core components, layered API hierarchy, and integration with tools like Keras for efficient model building. It explains the structure of TensorFlowâ€”from low-level operations to high-level model training APIsâ€”and emphasises the importance of designing scalable input data pipelines using the tf.data API. Users learn how to work with both in-memory and file-based datasets, apply preprocessing steps, and prepare data effectively for machine learning models, including the use of embeddings for representing categorical variables.

The hands-on sessions focus on building practical pipelines and models using TensorFlow. Learners apply the tf.data API for efficient data handling and combine it with Keras preprocessing layers for seamless integration of preprocessing into the model itself. The final lab guides users through building and training a classifier on structured data, demonstrating how to scale preprocessing and training in an end-to-end workflowâ€”ensuring consistency between training and inference environments.

## ğŸ“š Table of Contents

### Introduction to TensorFlow Ecosystem
1. Introduction to TensorFlow
2. TensorFlow API Hierarchy
3. Components of TensorFlow

### Design and Input Data Pipeline
1. Training on large datasets with tf.data API <br/>------------------------------------------------- Following topics are to be added/updated ------------------------------------------------- <br/>
2. Working in-memory and with files 
3.  Getting Data ready for the model
4.  Embeddings
5.  Hands On: TensorFlow Dataset API
6.  Scaling Data Processing with tf.data & Keras preprocessing Layers
7.  Hands On: Classifying Structured Data using Keras Preprocessing Layers

---

# Professional Machine Learning Engineer - Engineer Data for Predictive Modelling with BigQuery ML Part 1

ğŸ“˜ <a href='https://fern-stop-81f.notion.site/ML-Engineering-Path-Engineer-Data-for-Predictive-Modeling-with-BigQuery-ML-1e213f9f5c0380ab867ee9f39c935619'> Read Here </a> 

## ğŸ—’ï¸ Summary
This module provides a practical walkthrough of building and executing data transformation pipelines using Cloud Dataprep and Google Cloud Dataflow with BigQuery (Python). It begins with setting up a BigQuery dataset and integrating it with Dataprep for interactive data exploration, cleaning, and enrichment using a visual interface. Users then run transformation jobs that output to BigQuery. The second part focuses on enabling the Dataflow API and working with a code-driven ETL process, covering ingestion, transformation, enrichment, and creating a full pipeline from data lake to data mart, leveraging Cloud Storage and Dataflow for scalable data operations.

## ğŸ“š Table of Contents

### Creating a Data Transformation Pipeline with Cloud Dataprep
1. Task 1: Open Dataprep in the Google Cloud Console
2. Task 2: Create a BigQuery dataset
3. Task 3: Connecting BigQuery Data to Cloud Dataprep
4. Tasks 4:Exploring the ecommerce dataset's fields with UI
5. Tasks 5: Cleaning Data
6. Task 6: Enriching the Data
7. Running Cloud Dataprep jobs to BigQuery

### ETL Processing on Google Cloud Using Dataflow and BigQuery (Python)
1. Enable Dataflow API
2. Task 1: Download Starter Code
3. Task 2: Create a Cloud Storage bucket and copy files to the bucket
4. Task 3: Review & run data ingestion pipeline
5. Task 4: Review & run data transformation pipeline
6. Task 5: Review & run data enrichment pipeline
7. Task 6: Review & run data lake to data mart pipeline

---

# Machine Learning Engineer Learning Path - Responsible AI for Devs: Privacy & Safety

ğŸ“˜ <a href='https://fern-stop-81f.notion.site/Machine-Learning-Engineer-Learning-Path-Responsible-AI-for-Devs-Privacy-Safety-1d513f9f5c038034baf6e90a16130d4d?pvs=4'> Read Here </a> 

## ğŸ—’ï¸ Summary
This learning module explores the critical aspects of AI privacy and safety, beginning with the fundamental need for privacy in AI systems and progressing through key de-identification and randomized techniques that help protect sensitive data. It delves into advanced approaches like Differential Privacy with DP-SGD and Federated Learning, which enable secure, decentralized model training. The module also covers system-level security within Google Cloud and Generative AI environments. It concludes with AI safety evaluation methods and training strategies focused on responsible AI behavior, including instruction fine-tuning and Reinforcement Learning with Human Feedback (RLHF) to ensure models act safely and ethically.

## ğŸ“š Table of Contents
1. AI Privacy
2. What is the need of Privacy
3. De-Identification Techniques
4. Randomised Techniques
5. DP-SGD
6. Federated Learning
7. System Security on Google Cloud & Gen AI
8. AI Safety Evaluation
9. Model Training for Safety - Instruction Fine-Tuning
10. Model Training for Safety - RLHF
