# Machine Learning Operations (MLOps) with Vertex AI : Model Evaluation

üìò <a href='#'> Link Coming soon ...</a> 


## üóíÔ∏è Summary
These notes provide a comprehensive overview of the evolution and application of MLOps, with a specific focus on the critical discipline of model evaluation, particularly for Generative AI.
The document begins by tracing the evolution of MLOps, contextualizing its development from supporting traditional predictive AI to adapting to the new paradigm of generative AI.
It then transitions to the core topic of model evaluation, establishing it as a fundamental component of the MLOps lifecycle. The notes introduce the common challenges associated with model evaluation and present Google‚Äôs Vertex AI as a platform offering robust solutions to overcome these hurdles.
The main focus of the notes is a deep dive into model evaluation specifically for Generative AI. This section addresses the unique challenges posed by Large Language Models (LLMs) and frames their evaluation as both an art and a science. It emphasizes the need to move **beyond simple accuracy metrics** to a more holistic set of evaluation criteria. The notes outline **best practices for LLM evaluation** and detail two key methodologies:

1. **Computation-based Metrics**: Streamlining evaluation through objective, automated calculations. 
2. **Model-based Evaluation**: Comparing the performance of a candidate model against a benchmark or reference model.

Overall, the notes serve as a guide to navigating the complexities of modern model evaluation, from foundational principles to the advanced techniques required to effectively assess today‚Äôs sophisticated generative models.

## üìö Table of Contents

1. The Evolution of MLOps 
   1. MLOps
   2. Predictive AI
   3. Generative AI
2. Introduction of Model Evaluation
   1. Model Evaluation with MLOps
   2. Model Evaluation Challenges and Solutions offered by Vertex AI
3. Model Evaluation for Generative AI
   1. Challenges of evaluating the Gen AI Tasks
   2. The Art and Science of Evaluating LLMs
   3. Beyond Accuracy: Mastering Evaluation Metrics for Gen AI
   4. Best Practices for LLM Evaluation
   5. Solving Evaluation Challenges
   6. Streamlining Model evaluation with computation based Metrics
   7. Comparnig performances with Model based evaluation
   